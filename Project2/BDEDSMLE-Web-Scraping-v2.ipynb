{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Knowing Beautiful Soup and how to select various elements from a web page, it's time to practice scraping a website. We'll start to see that scraping is a dynamic process that involves investigating the web page(s) at hand and developing scripts tailored to those structures.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this analysis, we plan to:\n",
    "\n",
    "* Navigate HTML documents using Beautiful Soup's children and sibling relations\n",
    "\n",
    "* Select specific elements from HTML using Beautiful Soup\n",
    "\n",
    "* Use regular expressions to extract items with a certain pattern within Beautiful Soup\n",
    "\n",
    "* Determine the pagination scheme of a website and scrape multiple pages\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis will build upon a script that will iterate over all of the pages for the demo site and extract the job title, company name, company rating, company location (city included) and job description summary of each job AD posted. Building up to that, we'll formalize the concepts by writing functions that will extract a list of each of these features for each web page. We'll then combine these functions into the full script which will look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, 11):\n",
    "\n",
    "    url = \"https://www.indeed.com/jobs?q=big+data+developer&l=Denver&start={}\".format(i)\n",
    "\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "    new_place = retrieve_comLocat(soup)\n",
    "    \n",
    "    new_names = retrieve_jobTitle(soup)\n",
    "    \n",
    "    new_comps = retrieve_comNames(soup)\n",
    "    \n",
    "    new_rates = retrieve_comRates(soup)\n",
    "    \n",
    "    new_jobad = retrieve_jobConts(soup)\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing an HTML Page\n",
    "\n",
    "To start, here's how to retrieve an arbitrary web page and load its content into Beautiful Soup for parsing. You first use the requests package to pull the HTML itself and then pass that data to beautiful soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = requests.get('https://www.indeed.com/jobs?q=big+data+engineer&l=Chicago&start=0') # Make a get request to retrieve the page\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') # Pass the page contents to beautiful soup for parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing the Structure\n",
    "\n",
    "While it's apt to be too much information to effectively navigate, taking a quick peek into the structure of the HTML page is always a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.prettify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Container\n",
    "\n",
    "While we're eventually looking to select each of the individual jobs, it's often easier to start with an encapsulating container. In this case, the section displayed above. Once we select this container, we can then make sub-selections within it to find the relevant information we are searching for. In this case, the warning just above the div for the jobs is easy to identify. We can start by selecting this element and then navigating to the next div element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_container = soup.find_all(name=\"div\", attrs={\"class\" : \"row\"})\n",
    "job_container # Previewing is optional but can help you verify you are selecting what you think you are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results_per_city = 1\n",
    "city_set = ['Raleigh', 'Boston', 'Portland', 'San+Diego', 'San+Francisco', 'Dallas', 'Denver', 'Hartford', 'Atlanta', \n",
    "            'Chicago', 'New+York', 'New+Jersey', 'Washington+DC', 'Tampa', 'Houston', 'Phoenix', 'philadelphia']\n",
    "columns = [\"job_title\", \"company_name\", \"company_rating\", \"company_location\", \"job_descriptions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Company Locations\n",
    "\n",
    "write a function that extracts locations on a given page. The input for the function should be the soup for the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comLocat(soup):\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    job_container = soup.find_all(name=\"div\", attrs={\"class\" : \"row\"})\n",
    "    \n",
    "    for div in job_container:\n",
    "        sjcl = div.find(name=\"div\", attrs={\"class\" : \"sjcl\"})\n",
    "        locs = sjcl.find_all(name=\"div\", attrs={\"class\" : \"location\"}) \n",
    "        if len(locs) > 0:\n",
    "            for c in locs:\n",
    "                locations.append(c.text)\n",
    "        else:\n",
    "            locations.append(\"N/A\")\n",
    "        \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Titles\n",
    "\n",
    "write a function that extracts job titles on a given page. The input for the function should be the soup for the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_jobTitle(soup):\n",
    "    \n",
    "    titles = []\n",
    "    \n",
    "    job_container = soup.find_all(name=\"div\", attrs={\"class\" : \"row\"})\n",
    "    \n",
    "    for div in job_container:\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\" : \"jobTitle\"}):\n",
    "            titles.append(a[\"title\"])\n",
    "            \n",
    "    return titles            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Company Names\n",
    "\n",
    "write a function that extracts company names on a given page. The input for the function should be the soup for the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comNames(soup):\n",
    "    \n",
    "    companies = []\n",
    "    \n",
    "    job_container = soup.find_all(name=\"div\", attrs={\"class\" : \"row\"})\n",
    "    \n",
    "    for div in job_container:\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\" : \"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            companies.append(\"N/A\")\n",
    "            \n",
    "    return companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Company Rates\n",
    "\n",
    "write a function that extracts company rates on a given page. The input for the function should be the `soup` for the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comRates(soup):\n",
    "    \n",
    "    ratings = []\n",
    "    \n",
    "    job_container = soup.find_all(name=\"div\", attrs={\"class\" : \"row\"})\n",
    "    \n",
    "    for div in job_container:\n",
    "        sjcl = div.find(name=\"div\", attrs={\"class\" : \"sjcl\"})\n",
    "        rating = sjcl.find_all(name=\"span\", attrs={\"class\" : \"ratingsContent\"})\n",
    "        if len(rating) > 0:\n",
    "            for d in rating:\n",
    "                ratings.append(d.text.strip())\n",
    "        else:\n",
    "            ratings.append(\"N/A\")\n",
    "            \n",
    "    return ratings        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Job Descriptions\n",
    "\n",
    "write a function that extracts job descriptions on a given page. The input for the function should be the `soup` for the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_jobConts(soup):\n",
    "    \n",
    "    descriptions = []\n",
    "    \n",
    "    job_container = soup.find_all(name=\"div\", attrs={\"class\" : \"row\"})\n",
    "    \n",
    "    for div in job_container:\n",
    "        try:\n",
    "            div_one = div.find(\"summary\").text\n",
    "            descriptions.append(div_one)\n",
    "        except:\n",
    "            try:\n",
    "                div_two = div.find(name=\"ul\", attrs={\"style\":\"list-style-type:circle;margin-top: 0px;margin-bottom: 0px;padding-left:20px;\"})\n",
    "                div_three = div_two.find(\"li\")\n",
    "                descriptions.append(div_three.text.strip())\n",
    "            except:\n",
    "                descriptions.append(\"N/A\")\n",
    "                \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_ads_by_title(title):\n",
    "    \n",
    "    jobTitle = []\n",
    "    compName = []\n",
    "    rateNumb = []\n",
    "    compLoct = []\n",
    "    jobsPost = []\n",
    "    \n",
    "    for city in city_set:\n",
    "        print(city)\n",
    "        for i in range(0, max_results_per_city, 1):\n",
    "            url = \"https://www.indeed.com/jobs?q={0}&l={1}&start={2}\".format(title, city, str(i))\n",
    "            html_page = requests.get(url)\n",
    "            soup = BeautifulSoup(html_page.text, \"html.parser\")\n",
    "            \n",
    "            new_names = retrieve_jobTitle(soup)\n",
    "            new_comps = retrieve_comNames(soup)\n",
    "            new_rates = retrieve_comRates(soup)\n",
    "            new_place = retrieve_comLocat(soup)\n",
    "            new_jobad = retrieve_jobConts(soup)\n",
    "            \n",
    "            jobTitle += new_names\n",
    "            compName += new_comps\n",
    "            rateNumb += new_rates\n",
    "            compLoct += new_place\n",
    "            jobsPost += new_jobad\n",
    "    \n",
    "    df = pd.DataFrame([jobTitle, compName, rateNumb, compLoct, jobsPost]).transpose()\n",
    "    df.columns = columns\n",
    "    print(len(df))\n",
    "    print(df.head(10))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    big_data_engineer_df= job_ads_by_title(\"big+data+engineer\")     \n",
    "    big_data_engineer_df.to_csv(\"big_data_engineer_jobs.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    big_data_developer_df= job_ads_by_title(\"big+data+developer\")     \n",
    "    big_data_developer_df.to_csv(\"big_data_developer_jobs.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_engineer_df= job_ads_by_title(\"data+engineer\")     \n",
    "    data_engineer_df.to_csv(\"data_engineer_jobs.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
